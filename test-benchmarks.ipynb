{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File just for internal testing of the model and benchmarking with predefined questions. Runs the process with 140 questions.\n",
    "\n",
    "import re\n",
    "import json\n",
    "from utils.model_utils import answer_with_subqueries\n",
    "\n",
    "with open(\"./data/clean-all-printings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_cards = json.load(f)\n",
    "\n",
    "with open(\"./data/easy-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    easy_questions = json.load(f)\n",
    "with open(\"./data/hard-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_questions = json.load(f)\n",
    "with open(\"./data/extra-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    extra_questions = json.load(f)\n",
    "\n",
    "\n",
    "all_questions = easy_questions + hard_questions + extra_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a264bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = []\n",
    "judge_ruling_conflict_questions = []\n",
    "unclear_questions = []\n",
    "incorrect_questions = []\n",
    "\n",
    "for question in all_questions:\n",
    "\n",
    "    # extract card names from question text between brackets using regex and remove duplicates\n",
    "    card_names = list(set(re.findall(r'\\[([^\\]]+)\\]', question[\"text\"])))\n",
    "\n",
    "    cards_info = []\n",
    "\n",
    "    # if card_name exists in all_cards, append the card its info to cards_info\n",
    "    for card_name in card_names:\n",
    "        matching_cards = [card for card in all_cards if card_name.lower() == card[\"name\"].lower()]\n",
    "        if matching_cards:\n",
    "            cards_info.extend(matching_cards)\n",
    "        else:\n",
    "            print(f\"Card {card_name} not found in database!\")\n",
    "\n",
    "    response = answer_with_subqueries(question[\"text\"], cards_info)\n",
    "\n",
    "    gold = question[\"answer\"].strip().lower()\n",
    "    pred = response[\"single_word_answer\"].strip().lower()\n",
    "    if pred == \"depends\":\n",
    "        pred = \"yes\" # asume depends means yes for the sake of this benchmark\n",
    "\n",
    "    if pred == gold:\n",
    "        correct_answers.append({\"question\": question, \"response\": response})\n",
    "    elif pred == \"denied\":\n",
    "        judge_ruling_conflict_questions.append({\"question\": question, \"response\": response})\n",
    "    elif pred == \"unclear\":\n",
    "        unclear_questions.append({\"question\": question, \"response\": response})\n",
    "    else:\n",
    "        incorrect_questions.append({\"question\": question, \"response\": response})\n",
    "\n",
    "total_question_length = len(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e94d81",
   "metadata": {},
   "source": [
    "# Final Benchmark Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a2f8e",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "\n",
    "- MAX_SUBQUERIES = 5\n",
    "- TOP_K = 6\n",
    "- 28 minutes\n",
    "- 2nd Judge callings: 9\n",
    "- correct answers: 125/140\n",
    "- unclear questions: 2/140\n",
    "- incorrect answers: 13/140"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b7ea5",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "\n",
    "- MAX_SUBQUERIES = 5\n",
    "- TOP_K = 4\n",
    "- MODEL_HIGH_TEMPERATURE = 0.3\n",
    "- MODEL_LOW_TEMPERATURE = 0\n",
    "- MAX_CONTENT_CHUNKS = 15\n",
    "- 2nd Judge callings: 7\n",
    "- correct answers: 132/140\n",
    "- unclear questions: 0/140\n",
    "- incorrect answers: 8/140"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8be25",
   "metadata": {},
   "source": [
    "## Test 3\n",
    "\n",
    "- MAX_SUBQUERIES = 5\n",
    "- TOP_K = 4\n",
    "- MODEL_HIGH_TEMPERATURE = 0.3\n",
    "- MODEL_LOW_TEMPERATURE = 0s\n",
    "- MAX_CONTENT_CHUNKS = 15\n",
    "- 2nd Judge callings: 7\n",
    "- correct answers: 133/140\n",
    "- unclear questions: 1/140\n",
    "- incorrect answers: 6/140"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef88f1",
   "metadata": {},
   "source": [
    "## Test 4 (system prompt tweeks)\n",
    "\n",
    "- MAX_SUBQUERIES = 5\n",
    "- TOP_K = 8\n",
    "- MODEL_HIGH_TEMPERATURE = 0.3\n",
    "- MODEL_LOW_TEMPERATURE = 0\n",
    "- MAX_CONTENT_CHUNKS = 25\n",
    "- 2nd Judge callings: 5\n",
    "- correct answers: 135/140\n",
    "- unclear questions: 0/140\n",
    "- incorrect answers: 5/140\n",
    "- time: 35m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"correct answers: {len(correct_answers)}/{total_question_length}\")\n",
    "\n",
    "print(f\"judge ruling conflict: {len(judge_ruling_conflict_questions)}/{total_question_length}\")\n",
    "# for question in judge_ruling_conflict_questions:\n",
    "#     print(question)\n",
    "\n",
    "print(f\"unclear questions: {len(unclear_questions)}/{total_question_length}\")\n",
    "# for question in unclear_questions:\n",
    "#     print(question)\n",
    "\n",
    "print(f\"incorrect answers: {total_question_length - len(correct_answers) - len(judge_ruling_conflict_questions) - len(unclear_questions)}/{total_question_length}\")\n",
    "# for question in incorrect_questions:\n",
    "#     print(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ac883",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in incorrect_questions:\n",
    "    print(\"original question:\", question[\"question\"][\"text\"])\n",
    "    print(\"correct answer: \", question[\"question\"][\"answer\"])\n",
    "    print(\"reworded question: \", question[\"response\"][\"question\"])\n",
    "    print(\"single_word_answer: \", question[\"response\"][\"single_word_answer\"])\n",
    "    print(\"short_answer: \", question[\"response\"][\"short_answer\"])\n",
    "    print(\"full_explanation: \", question[\"response\"][\"full_explanation\"])\n",
    "    print(\"sources: \", question[\"response\"][\"sources\"])\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4fe96",
   "metadata": {},
   "source": [
    "## Common Question Issues:\n",
    "\n",
    "- blockers and bounce effects\n",
    "- concept of loops (automatic infinite or triggered)\n",
    "- golden rule priority\n",
    "- summoning sickness persisting even through exile/bounce effects\n",
    "- invoking a creature and puting it on the battlefield\n",
    "- player on 0 life and State Based Actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba22056",
   "metadata": {},
   "source": [
    "### Potential improvements\n",
    "\n",
    "- Improving embedding chunks by rule sections\n",
    "- Adding card Oracle text for subqueries generation\n",
    "- Creating a dictionary that suggests MTG key concepts based on terms used in query, card text or card type\n",
    "  - for example:\n",
    "    - attack, block, combat should point to: combat phase, summoning sickness, turn structure\n",
    "    - instants spells should point to: stack, priority.\n",
    "    - cards with abilities should point to: triggered abilities, stack.\n",
    "- When response deems que question unclear, the model can ask the user for info one extra time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
