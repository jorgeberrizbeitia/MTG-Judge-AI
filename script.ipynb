{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f40982b",
   "metadata": {},
   "source": [
    "MTG Judge RAG\n",
    "---------------------------------------------------\n",
    "This is a simple Python script for building an AI-powered MTG rules assistant\n",
    "using Retrieval-Augmented Generation (RAG) with OpenAI + FAISS.\n",
    "\n",
    "- Loads the Comprehensive Rules from a text file.\n",
    "- Splits rules into chunks.\n",
    "- Creates embeddings with OpenAI.\n",
    "- Stores them in ChromaDB for fast search (not using FAISS due to py versioning)\n",
    "- Lets you ask questions, retrieves relevant rules, and asks the LLM to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4fe92255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- IMPORTS --------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import chromadb\n",
    "\n",
    "from openai import OpenAI\n",
    "# client = OpenAI(api_key=OPENAI_API_KEY)   # don’t pass api_key explicitly\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import ast # to convert from string to dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f7039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBED_MODEL = \"text-embedding-3-large\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "CHROMA_DB_DIR = \"./chroma_db\"\n",
    "os.makedirs(CHROMA_DB_DIR, exist_ok=True) # to create folder if it doesn't exist\n",
    "RULES_FILE = \"./comprehensive-rules.txt\"\n",
    "CARDS_FILE = \"./clean-standard-cards.json\"\n",
    "CHUNK_SIZE = 700 # words approximation\n",
    "TOP_K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4a9002e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- INITIALIZATION --------\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "baf4fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER LOAD RULES --------\n",
    "def load_rules(path):\n",
    "    \"\"\"Load the MTG comprehensive rules from a text file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Rules file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    docs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Rules usually like: 603.1. Some text\n",
    "            match = re.match(r\"^(\\d{1,3}(?:\\.\\d+)+)\\s+(.*)$\", line)\n",
    "            if match:\n",
    "                rule_id, body = match.groups()\n",
    "                docs.append({\n",
    "                    \"id\": f\"CR:{rule_id}\",\n",
    "                    \"text\": f\"{rule_id} {body}\",\n",
    "                    \"rule_id\": rule_id,\n",
    "                    \"source\": \"Comprehensive Rules\"\n",
    "                })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb4bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER LOAD CARDS --------\n",
    "def load_cards(path):\n",
    "    \"\"\"Load MTG card data from your JSON export.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Card file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cards = json.load(f)\n",
    "\n",
    "    docs = []\n",
    "    for c in cards:\n",
    "        # Skip cards without names or text\n",
    "        if \"name\" not in c or not c.get(\"originalText\"):\n",
    "            continue\n",
    "\n",
    "        # Construct a searchable text block for embedding\n",
    "        text_parts = [\n",
    "            f\"Name: {c['name']}\",\n",
    "            f\"Mana Cost: {c.get('manaCost', '')}\",\n",
    "            f\"Types: {' '.join(c.get('types', []))}\",\n",
    "            f\"Subtypes: {' '.join(c.get('subtypes', []))}\",\n",
    "            f\"Abilities/Keywords: {', '.join(c.get('keywords', []))}\",\n",
    "            f\"Text: {c['originalText']}\"\n",
    "        ]\n",
    "\n",
    "        # Add rulings (big chunk but useful)\n",
    "        rulings = c.get(\"rulings\", [])\n",
    "        if rulings:\n",
    "            rulings_text = \" | \".join(r[\"text\"] for r in rulings if \"text\" in r)\n",
    "            text_parts.append(f\"Rulings: {rulings_text}\")\n",
    "\n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "\n",
    "        docs.append({\n",
    "            \"id\": f\"CARD:{c['uuid']}\",   # use UUID for uniqueness\n",
    "            \"text\": full_text,\n",
    "            \"source\": \"Card Database\",\n",
    "            \"card_name\": c[\"name\"],\n",
    "            \"manaCost\": c.get(\"manaCost\", \"\"),\n",
    "            \"types\": \", \".join(c.get(\"types\", [])),       # FIXED: stringify list\n",
    "            \"subtypes\": \", \".join(c.get(\"subtypes\", [])), # FIXED: stringify list\n",
    "            \"keywords\": \", \".join(c.get(\"keywords\", [])), # FIXED: stringify list\n",
    "            \"rarity\": c.get(\"rarity\", \"\")\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(docs)} cards from {path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e04fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER CHUNK TEXT --------\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Split text into smaller chunks so embeddings don't get too big.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current = []\n",
    "    length = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = len(s.split())\n",
    "        if length + tokens > chunk_size:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            length = tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            length += tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57272cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER BUILD INDEX --------\n",
    "def build_index():\n",
    "    \"\"\"Create ChromaDB collection from rules + card data.\"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    print(\"Loading rules...\")\n",
    "    rules = load_rules(RULES_FILE)\n",
    "\n",
    "    print(\"Loading cards...\")\n",
    "    cards = load_cards(CARDS_FILE)  # add this\n",
    "\n",
    "    all_docs = rules + cards  # merge datasets\n",
    "\n",
    "    texts, metas, ids = [], [], []\n",
    "\n",
    "    for d in all_docs:\n",
    "        chunks = chunk_text(d[\"text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            texts.append(ch)\n",
    "            metas.append(d)\n",
    "            ids.append(f\"{d['id']}_{i}\")\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid chunks found to embed.\")\n",
    "\n",
    "    print(f\"Total chunks: {len(texts)}\")\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    vecs = [d.embedding for d in embeddings.data]\n",
    "\n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "\n",
    "    # Drop old collection (clean rebuild)\n",
    "    try:\n",
    "        chroma_client.delete_collection(\"mtg_data\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_data\")\n",
    "\n",
    "    # Add to Chroma\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=vecs,\n",
    "        documents=texts,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    print(\"Index built and saved with ChromaDB!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "94b43337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER SEARCH INDEX --------\n",
    "def search_index(query, top_k=TOP_K):\n",
    "    \"\"\"Search ChromaDB for relevant rule chunks.\"\"\"\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        raise ValueError(\"Empty query provided.\")\n",
    "\n",
    "    # client = OpenAI()\n",
    "    emb = client.embeddings.create(model=EMBED_MODEL, input=[query])\n",
    "    vec = emb.data[0].embedding\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_rules\")\n",
    "\n",
    "    results = collection.query(query_embeddings=[vec], n_results=top_k)\n",
    "\n",
    "    docs = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        docs.append({\n",
    "            \"text\": doc,\n",
    "            \"meta\": results[\"metadatas\"][0][i]\n",
    "        })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9f62d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER GENERATE SUBQUERIES --------\n",
    "def generate_subqueries(query, n=10):\n",
    "    \"\"\"Chain of Thought decomposition function. Use the LLM to break a user query into smaller sub-questions.\"\"\"\n",
    "    #client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    prompt = f\"\"\"\n",
    "    Break down the following Magic: The Gathering rules question into {n} smaller, \n",
    "    more specific sub-questions that cover timing, abilities, rules interactions, \n",
    "    and possible edge cases. Return them as a numbered list.\n",
    "\n",
    "    Original Question: {query}\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert MTG judge assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    text = resp.choices[0].message.content\n",
    "    subqueries = [line.strip(\"0123456789. \") for line in text.splitlines() if line.strip()]\n",
    "    return subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "60d0491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER JSON PARSE --------\n",
    "def safe_json_parse(text):\n",
    "    \"\"\"Function to safely parse from string to json. Converts response from gpt to json\"\"\"\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to \"repair\" common JSON issues\n",
    "        fixed = text.strip()\n",
    "        if fixed.startswith(\"```\"):\n",
    "            fixed = fixed.split(\"```\")[1]  # strip markdown fences\n",
    "        try:\n",
    "            return json.loads(fixed)\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse JSON\", \"raw\": text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER ANSWER WITH SUBQUERIES --------\n",
    "def answer_with_subqueries(query):\n",
    "    \"\"\"Break question into subqueries, search index for each, and generate final answer.\"\"\"\n",
    "    # Step 1: Get subqueries\n",
    "    subqueries = generate_subqueries(query, n=10) #* 10 subqueries\n",
    "\n",
    "    # Step 2: Collect context from all subqueries\n",
    "    all_context = []\n",
    "    for sq in subqueries:\n",
    "        results = search_index(sq, top_k=5)  # use your existing search_index\n",
    "        for i, r in enumerate(results, 1):\n",
    "            all_context.append(f\"Subquery: {sq}\\n- Source: {r['meta'].get('source', '')}\\n- Text: {r['text']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(all_context)\n",
    "\n",
    "    response_format = \"\"\"\n",
    "    Please provide a structured answer as a single json file with the following properties:\n",
    "        \n",
    "    - \"question\": a string with a rephrased version of the user question in the most clear form,\n",
    "    - \"single_word_answer\": a string with a single word, either: \"Yes\", \"No\" or \"Unclear\" that defines the response of the user query\n",
    "    - \"short answer\": short paragraph with a summary of the answer\n",
    "    - \"full_explanation\": a string with the detailed reasoning with rules and card interactions,\n",
    "    - \"sources: a string\" with that cites the full text of rules used for the response and also the card text used for the reasoning.\n",
    "\n",
    "    No other text should be provided, just the json structure.\n",
    "\n",
    "    IMPORTANT. Asume that the user might have unclear, missing or conflicting information. If that happends feel free to let the user know. You don't need to force a yes or no answer. You can say that you are unclear and need a query that is more clear. Even in this case, use the above provided structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Ask LLM for final structured response\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert Magic: The Gathering judge assistant.\n",
    "    A user has a question about card interactions or rules.\n",
    "\n",
    "    Use these sources (rules + card texts):\n",
    "    {context}\n",
    "\n",
    "    Answer format:\n",
    "    {response_format}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    The question from the user is:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    client1 = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    resp = client1.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0, # lowering temperature for more accurate and consistant response according to rules\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #* SECONDARY JUDGE VERIFICATION\n",
    "    judge2_system_prompt = f\"\"\"\n",
    "    You are an expert Magic: The Gathering high judge.\n",
    "    You will be given:\n",
    "\n",
    "    - A user’s question\n",
    "    - A judge’s ruling on that question\n",
    "    - All the context provided by the judge\n",
    "\n",
    "    Your task is to carefully review and analyze the information. Then, determine whether the context adequately supports the judge’s ruling. Based on your analysis, you may either accept or deny the ruling.\n",
    "\n",
    "    Your response should follow this format:\n",
    "\n",
    "    - If you agree with the ruling, respond: Accepted\n",
    "    - If you disagree with the ruling, respond: Denied, [include context explaining the reason for denial, also add recommendations for more context to extract from the rules]\n",
    "\n",
    "    The original user question is:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    judge_prompt = f\"\"\"\n",
    "    The context brought by the initial ruling judge was:\n",
    "    {context}\n",
    "\n",
    "    The final response from the initial judge was:\n",
    "    {resp.choices[0].message.content}\n",
    "    \"\"\"\n",
    "\n",
    "    client2 = OpenAI(api_key=OPENAI_API_KEY) # new model initialization for new judge\n",
    "    resp2 = client2.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0, # lowering temperature for more accurate and consistant response according to rules\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge2_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": judge_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    judge2_response = resp2.choices[0].message.content\n",
    "\n",
    "    if judge2_response.strip().startswith(\"Accepted\"):\n",
    "        return ast.literal_eval(resp.choices[0].message.content) # converts string to dict\n",
    "    \n",
    "    #* if judge 2 denied the ruling, ask judge 1 to look for more information and check ruling again\n",
    "    new_subqueries = generate_subqueries(judge2_response, n=10) #* 10 subqueries\n",
    "\n",
    "    # Step 2: Collect context from all subqueries\n",
    "    all_context = []\n",
    "    for sq in new_subqueries:\n",
    "        results = search_index(sq, top_k=5)  # use your existing search_index\n",
    "        for i, r in enumerate(results, 1):\n",
    "            all_context.append(f\"Subquery: {sq}\\n- Source: {r['meta'].get('source', '')}\\n- Text: {r['text']}\")\n",
    "\n",
    "    context = context + \"\\n\\n\".join(all_context)\n",
    "\n",
    "    new_prompt = f\"\"\"\n",
    "    A judge just ruled your response invalid due to lack of context. Please use the provided context for a better understanding of the rule and come up with a better response.\n",
    "\n",
    "    secondary judge response:\n",
    "    {judge2_response}\n",
    "\n",
    "    new context provided:\n",
    "    {context}\n",
    "\n",
    "    use the same structe as requested in the system prompt\n",
    "    \"\"\"\n",
    "\n",
    "    resp3 = client1.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0, # lowering temperature for more accurate and consistant response according to rules\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": resp.choices[0].message.content},\n",
    "            {\"role\": \"user\", \"content\": new_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return ast.literal_eval(resp3.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # return {\n",
    "        #     \"single_word_answer\": \"Denied\",\n",
    "        #     \"question\": query,\n",
    "        #     \"full_explanation\": f\"\"\"\n",
    "        #         We’re sorry, our virtual judges were unable to reach an agreement on a final response to your question.\n",
    "\n",
    "        #         Reason: The original judge’s ruling was: {judge2_response}\n",
    "\n",
    "        #         Please try asking your question again, this time with clearer wording to help us provide a more definitive answer.\n",
    "        #     \"\"\"\n",
    "        # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "07ad31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rules...\n",
      "Loading cards...\n",
      "Loaded 90 cards from ./clean-standard-cards.json\n",
      "Total chunks: 91\n",
      "Index built and saved with ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# -------- BUILDING INDEX --------\n",
    "build_index()  # only first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c76f5f",
   "metadata": {},
   "source": [
    "# Single testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46b4e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?\",\n",
      "    \"single_word_answer\": \"No\",\n",
      "    \"short_answer\": \"There cannot be infinite or multiple cleanup steps triggered by effects like Kozilek's ability. The cleanup step is a specific phase in the turn structure that occurs once, and while triggered abilities can occur during this step, they do not create additional cleanup steps.\",\n",
      "    \"full_explanation\": \"In Magic: The Gathering, the cleanup step is a defined part of the turn structure that occurs after the end step. During the cleanup step, players discard down to their maximum hand size, and any abilities that trigger at this time resolve. However, the cleanup step itself does not repeat or create additional cleanup steps. Kozilek, Butcher of Truth has a triggered ability that allows you to draw cards when you discard, but this does not lead to an infinite loop or multiple cleanup steps. The rules state that the cleanup step occurs only once per turn, and while triggered abilities can activate during this phase, they do not alter the structure of the turn or create new cleanup steps. Therefore, even if multiple Kozilek cards are in play, they will not create additional cleanup steps, and the cleanup step will still only occur once.\",\n",
      "    \"sources\": \"Comprehensive Rules, specifically rule 606.5: 'If the total cost to activate a loyalty ability contains multiple costs to add or remove loyalty counters, those costs are combined into a single cost to add or remove loyalty counters, as appropriate.'\"\n",
      "}\n",
      "{'single_word_answer': 'Ruling Denied', 'question': 'Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?', 'full_explanation': \"\\n                We’re sorry, our virtual judges were unable to reach an agreement on a final response to your question.\\n\\n                Reason: The original judge’s ruling was: Denied, the context provided does not adequately support the judge's ruling. The ruling states that the cleanup step occurs only once per turn and that triggered abilities during this step do not create additional cleanup steps. However, the references to rule 606.5 regarding loyalty abilities are not relevant to the cleanup step or the interaction of Kozilek's ability with discard effects. The cleanup step is indeed a defined phase, but the explanation lacks clarity on how triggered abilities specifically interact with the cleanup step and does not address the potential for infinite loops created by specific card interactions. The ruling should have included a more thorough examination of the rules governing triggered abilities and their resolution during the cleanup step.\\n\\n                Please try asking your question again, this time with clearer wording to help us provide a more definitive answer.\\n            \"}\n"
     ]
    }
   ],
   "source": [
    "single_question = \"Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?\"\n",
    "response = answer_with_subqueries(single_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd13c42",
   "metadata": {},
   "source": [
    "# Multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9c00ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"Can imprinting Time Walk on Panoptic Mirror lead to infinite turns?\",\n",
      "    \"single_word_answer\": \"Unclear\",\n",
      "    \"short_answer\": \"While imprinting Time Walk on Panoptic Mirror allows you to take an extra turn, achieving infinite turns requires additional interactions or cards that enable multiple activations of Panoptic Mirror within the same turn or during extra turns.\",\n",
      "    \"full_explanation\": \"Imprinting Time Walk on Panoptic Mirror allows you to activate its ability to cast Time Walk, which grants you an extra turn. However, you can only activate Panoptic Mirror's ability once per turn. To achieve infinite turns, you would need to find a way to activate Panoptic Mirror multiple times in a single turn or during the extra turns gained from Time Walk. This could involve using other cards that allow you to untap or reuse Panoptic Mirror, or cards that grant additional actions or turns. Without such interactions, you cannot achieve infinite turns solely with Panoptic Mirror and Time Walk. Therefore, while it is possible to gain extra turns, the conditions for infinite turns are not met without additional support.\",\n",
      "    \"sources\": \"Comprehensive Rules, 606.5\"\n",
      "}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_dict)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# response = answer_with_subqueries(question.text)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresponse_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msingle_word_answer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m question[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     27\u001b[0m     correct_answers_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_word_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDenied\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "with open(\"easy-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    easy_questions = json.load(f)\n",
    "with open(\"hard-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_questions = json.load(f)\n",
    "with open(\"own-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    own_questions = json.load(f)\n",
    "\n",
    "# all_questions = easy_questions + hard_questions + own_questions\n",
    "all_questions = own_questions\n",
    "\n",
    "correct_answers_count = 0\n",
    "\n",
    "judge_ruling_conflict_count = 0\n",
    "judge_ruling_conflict_questions = []\n",
    "\n",
    "unclear_questions_count = 0\n",
    "unclear_questions = []\n",
    "\n",
    "wrong_answered_questions = []\n",
    "\n",
    "for question in all_questions:\n",
    "    # response = answer_question(question[\"text\"])\n",
    "    response_dict = answer_with_subqueries(question[\"text\"])\n",
    "    print(response_dict)\n",
    "    # response = answer_with_subqueries(question.text)\n",
    "    if response_dict[\"single_word_answer\"] == question[\"answer\"]:\n",
    "        correct_answers_count += 1\n",
    "    elif response_dict[\"single_word_answer\"] == \"Denied\":\n",
    "        judge_ruling_conflict_count += 1\n",
    "        judge_ruling_conflict_questions.append({\"question\": question, \"response_dict\": response_dict})\n",
    "    elif response_dict[\"single_word_answer\"] == \"Unclear\":\n",
    "        unclear_questions_count += 1\n",
    "        unclear_questions.append({\"question\": question, \"response_dict\": response_dict})\n",
    "    else:\n",
    "        wrong_answered_questions.append({\"question\": question, \"response_dict\": response_dict})\n",
    "\n",
    "print(f\"correct answers: {correct_answers_count}/{len(all_questions)}\")\n",
    "\n",
    "print(f\"judge ruling conflict: {judge_ruling_conflict_count}/{len(all_questions)}\")\n",
    "for question in judge_ruling_conflict_questions:\n",
    "    print(question)\n",
    "\n",
    "print(f\"judge ruling conflict: {unclear_questions_count}/{len(all_questions)}\")\n",
    "for question in unclear_questions:\n",
    "    print(question)\n",
    "\n",
    "print(f\"incorrect answers: {len(all_questions)-correct_answers_count-judge_ruling_conflict_count-unclear_questions_count}/{len(all_questions)}\")\n",
    "for question in wrong_answered_questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d4e00",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812b32e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Test: No temperature set and no subqueries.\n",
    "\n",
    "### Correct answers: 30/45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2a0e5",
   "metadata": {},
   "source": [
    "## Test: Temperature set to 0.1 and no subqueries.\n",
    "\n",
    "### Correct answers: 35/45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33259b",
   "metadata": {},
   "source": [
    "## Test: Temperature set to 0.1 and 5 subqueries.\n",
    "\n",
    "### Correct answers: 37/45\n",
    "\n",
    "#### Incorrect answer to:\n",
    "- Does scry let you look at cards and choose to put some on top or bottom of your library?\n",
    "- If you draw more than seven cards, can you keep them all if no other effect limits your hand size?\n",
    "- Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?\n",
    "- If a creature phases out and back in, does it lose summoning sickness if it had it before?\n",
    "- Are special actions like playing a land or turning a face-down creature face-up things opponents cannot respond to?\n",
    "- If I imprint Time Walk on Panoptic Mirror, do I get infinite turns?\n",
    "- I am being attacked by Axebane Ferox and I declare Aegis Turtle as a blocker. But before assigning damage, I play Bounce Off on Aegis Turtle. Do I still receive 4 damage from Agonasaur Rex?\n",
    "- Someone is playing Flashfreeze on one of my spells, Can I play Aven Interrupter on top of Flashfreeze so my initial spell can be resolved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5de3c",
   "metadata": {},
   "source": [
    "## Test: Temperature set to 0 and 10 subqueries.\n",
    "\n",
    "- 8 minutes for 45 queries (10 seconds per query)\n",
    "\n",
    "### Correct answers: 39/45\n",
    "\n",
    "#### Incorrect answer to:\n",
    "- Does scry let you look at cards and choose to put some on top or bottom of your library?. Right answer: Yes. Response: No\n",
    "- Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?. Right answer: Yes. Response: No\n",
    "- Are continuous effects applied in a specific layered system, such as type-changing, ability additions, P/T changes, in numbered order?. Right answer: Yes. Response: No\n",
    "- Are special actions like playing a land or turning a face-down creature face-up things opponents cannot respond to?. Right answer: Yes. Response: No\n",
    "- If I imprint Time Walk on Panoptic Mirror, do I get infinite turns?. Right answer: Yes. Response: No\n",
    "- Someone is playing Flashfreeze on one of my spells, Can I play Aven Interrupter on top of Flashfreeze so my initial spell can be resolved?. Right answer: Yes. Response: No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e27f3c",
   "metadata": {},
   "source": [
    "## Test: Temperature set to 0, 10 subqueries, top_k set to 5 (previously 3) and added secondary judge. \n",
    "\n",
    "- 8 minutes for 45 queries (10 seconds per query)\n",
    "\n",
    "### Correct answers: 39/45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
