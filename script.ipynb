{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f40982b",
   "metadata": {},
   "source": [
    "MTG Judge RAG\n",
    "---------------------------------------------------\n",
    "This is a simple Python script for building an AI-powered MTG rules assistant\n",
    "using Retrieval-Augmented Generation (RAG) with OpenAI + FAISS.\n",
    "\n",
    "- Loads the Comprehensive Rules from a text file.\n",
    "- Splits rules into chunks.\n",
    "- Creates embeddings with OpenAI.\n",
    "- Stores them in ChromaDB for fast search (not using FAISS due to py versioning)\n",
    "- Lets you ask questions, retrieves relevant rules, and asks the LLM to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fe92255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- IMPORTS --------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import chromadb\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()   # donâ€™t pass api_key explicitly\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f7039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBED_MODEL = \"text-embedding-3-large\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "CHROMA_DB_DIR = \"./chroma_db\"\n",
    "os.makedirs(CHROMA_DB_DIR, exist_ok=True) # to create folder if it doesn't exist\n",
    "RULES_FILE = \"./ComprehensiveRules.txt\"\n",
    "CHUNK_SIZE = 700 # words approximation\n",
    "TOP_K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57272cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(path):\n",
    "    \"\"\"Load the MTG comprehensive rules from a text file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Rules file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    docs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Rules usually like: 603.1. Some text\n",
    "            match = re.match(r\"^(\\d{1,3}(?:\\.\\d+)+)\\s+(.*)$\", line)\n",
    "            if match:\n",
    "                rule_id, body = match.groups()\n",
    "                docs.append({\n",
    "                    \"id\": f\"CR:{rule_id}\",\n",
    "                    \"text\": f\"{rule_id} {body}\",\n",
    "                    \"rule_id\": rule_id,\n",
    "                    \"source\": \"Comprehensive Rules\"\n",
    "                })\n",
    "    return docs\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Split text into smaller chunks so embeddings don't get too big.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current = []\n",
    "    length = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = len(s.split())\n",
    "        if length + tokens > chunk_size:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            length = tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            length += tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def build_index():\n",
    "    \"\"\"Create ChromaDB collection from the rules and generate embeddings.\"\"\"\n",
    "    client = OpenAI()  # new SDK picks up API key from environment\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_rules\")\n",
    "\n",
    "    # Clear previous data\n",
    "    chroma_client.delete_collection(\"mtg_rules\")  # remove old collection\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_rules\")  # recreate empty collection\n",
    "\n",
    "    print(\"Loading rules...\")\n",
    "    rules = load_rules(RULES_FILE)\n",
    "\n",
    "    texts = []\n",
    "    metas = []\n",
    "    ids = []\n",
    "\n",
    "    for r in rules:\n",
    "        chunks = chunk_text(r[\"text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            ch = ch.strip()\n",
    "            if ch:  # only non-empty\n",
    "                texts.append(ch)\n",
    "                metas.append(r)\n",
    "                ids.append(f\"{r['id']}_{i}\")\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid rule chunks found to embed.\")\n",
    "\n",
    "    print(f\"Total chunks: {len(texts)}\")\n",
    "\n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    vecs = [d.embedding for d in embeddings.data]\n",
    "\n",
    "    # Add to ChromaDB\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=vecs,\n",
    "        documents=texts,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    print(\"Index built and saved with ChromaDB!\")\n",
    "\n",
    "def search_index(query, top_k=TOP_K):\n",
    "    \"\"\"Search ChromaDB for relevant rule chunks.\"\"\"\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        raise ValueError(\"Empty query provided.\")\n",
    "\n",
    "    client = OpenAI()\n",
    "    emb = client.embeddings.create(model=EMBED_MODEL, input=[query])\n",
    "    vec = emb.data[0].embedding\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_rules\")\n",
    "\n",
    "    results = collection.query(query_embeddings=[vec], n_results=top_k)\n",
    "\n",
    "    docs = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        docs.append({\n",
    "            \"text\": doc,\n",
    "            \"meta\": results[\"metadatas\"][0][i]\n",
    "        })\n",
    "    return docs\n",
    "\n",
    "def answer_question(query):\n",
    "    \"\"\"Retrieve context and ask the LLM for an answer.\"\"\"\n",
    "    results = search_index(query)\n",
    "    context_blocks = []\n",
    "\n",
    "    for i, r in enumerate(results, 1):\n",
    "        text = r[\"text\"]\n",
    "        meta = r[\"meta\"]\n",
    "        context_blocks.append(f\"[{i}] {text}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    user_prompt = f\"Question: {query}\\n\\nUse these sources:\\n{context}\\n\\nAnswer:\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert MTG judge assistant. Cite rule numbers when possible.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "479cbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rules...\n",
      "Total chunks: 1\n",
      "Creating embeddings...\n",
      "Index built and saved with ChromaDB!\n",
      "To determine if imprinting *Time Walk* on *Panoptic Mirror* grants you infinite turns, we need to analyze how both cards interact according to the rules.\n",
      "\n",
      "*Panoptic Mirror* has the ability to imprint a sorcery card and allows you to pay 2 colorless mana and tap it to cast the imprinted card, which is *Time Walk* in this case.\n",
      "\n",
      "When you cast *Time Walk* from *Panoptic Mirror*, it allows you to take an extra turn after the current one. The important detail here is that after that extra turn is completed, *Time Walk* is put into the graveyard (as per the card's rules), and you cannot activate *Panoptic Mirror* again without imprinted a new spell (or the same card again).\n",
      "\n",
      "To clarify the situation:\n",
      "\n",
      "1. When you cast *Time Walk* via *Panoptic Mirror*, you take an extra turn.\n",
      "2. After the extra turn ends, *Time Walk* goes to the graveyard.\n",
      "3. You cannot use *Panoptic Mirror* to cast *Time Walk* again unless you have another one to imprint, since the original is no longer in your deck.\n",
      "4. You will only get one extra turn per casting of *Time Walk*.\n",
      "\n",
      "Thus, the conclusion is **no**, you do not get infinite turns from imprinting *Time Walk* on *Panoptic Mirror*. You can only take one extra turn per activation of the mirror and only as long as *Time Walk* remains imprinted. \n",
      "\n",
      "This directly corresponds with the functionality of the cards as outlined by rule 707.2, which states that when a spell resolves, it carries out its effect and then goes to the graveyard. The ability of *Panoptic Mirror* does not create additional instances of *Time Walk*; instead, it allows you to reuse it a single time each activation.\n"
     ]
    }
   ],
   "source": [
    "# -------- TESTING --------\n",
    "build_index()  # only first time\n",
    "\n",
    "question = \"If I imprint Time Walk on Panoptic Mirror, do I get infinite turns?\"\n",
    "print(answer_question(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
