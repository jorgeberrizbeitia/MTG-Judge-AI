{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f40982b",
   "metadata": {},
   "source": [
    "MTG Judge RAG\n",
    "---------------------------------------------------\n",
    "This is a simple Python script for building an AI-powered MTG rules assistant\n",
    "using Retrieval-Augmented Generation (RAG) with OpenAI + FAISS.\n",
    "\n",
    "- Loads the Comprehensive Rules from a text file.\n",
    "- Splits rules into chunks.\n",
    "- Creates embeddings with OpenAI.\n",
    "- Stores them in ChromaDB for fast search (not using FAISS due to py versioning)\n",
    "- Lets you ask questions, retrieves relevant rules, and asks the LLM to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fe92255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- IMPORTS --------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import chromadb\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()   # donâ€™t pass api_key explicitly\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f7039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBED_MODEL = \"text-embedding-3-large\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "CHROMA_DB_DIR = \"./chroma_db\"\n",
    "os.makedirs(CHROMA_DB_DIR, exist_ok=True) # to create folder if it doesn't exist\n",
    "RULES_FILE = \"./comprehensive-rules.txt\"\n",
    "CARDS_FILE = \"./clean-standard-cards.json\"\n",
    "CHUNK_SIZE = 700 # words approximation\n",
    "TOP_K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER LOAD RULES --------\n",
    "def load_rules(path):\n",
    "    \"\"\"Load the MTG comprehensive rules from a text file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Rules file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    docs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Rules usually like: 603.1. Some text\n",
    "            match = re.match(r\"^(\\d{1,3}(?:\\.\\d+)+)\\s+(.*)$\", line)\n",
    "            if match:\n",
    "                rule_id, body = match.groups()\n",
    "                docs.append({\n",
    "                    \"id\": f\"CR:{rule_id}\",\n",
    "                    \"text\": f\"{rule_id} {body}\",\n",
    "                    \"rule_id\": rule_id,\n",
    "                    \"source\": \"Comprehensive Rules\"\n",
    "                })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER LOAD CARDS --------\n",
    "def load_cards(path):\n",
    "    \"\"\"Load MTG card data from your JSON export.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Card file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cards = json.load(f)\n",
    "\n",
    "    docs = []\n",
    "    for c in cards:\n",
    "        # Skip cards without names or text\n",
    "        if \"name\" not in c or not c.get(\"originalText\"):\n",
    "            continue\n",
    "\n",
    "        # Construct a searchable text block for embedding\n",
    "        text_parts = [\n",
    "            f\"Name: {c['name']}\",\n",
    "            f\"Mana Cost: {c.get('manaCost', '')}\",\n",
    "            f\"Types: {' '.join(c.get('types', []))}\",\n",
    "            f\"Subtypes: {' '.join(c.get('subtypes', []))}\",\n",
    "            f\"Abilities/Keywords: {', '.join(c.get('keywords', []))}\",\n",
    "            f\"Text: {c['originalText']}\"\n",
    "        ]\n",
    "\n",
    "        # Add rulings (big chunk but useful)\n",
    "        rulings = c.get(\"rulings\", [])\n",
    "        if rulings:\n",
    "            rulings_text = \" | \".join(r[\"text\"] for r in rulings if \"text\" in r)\n",
    "            text_parts.append(f\"Rulings: {rulings_text}\")\n",
    "\n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "\n",
    "        docs.append({\n",
    "            \"id\": f\"CARD:{c['uuid']}\",   # use UUID for uniqueness\n",
    "            \"text\": full_text,\n",
    "            \"source\": \"Card Database\",\n",
    "            \"card_name\": c[\"name\"],\n",
    "            \"manaCost\": c.get(\"manaCost\", \"\"),\n",
    "            \"types\": \", \".join(c.get(\"types\", [])),       # FIXED: stringify list\n",
    "            \"subtypes\": \", \".join(c.get(\"subtypes\", [])), # FIXED: stringify list\n",
    "            \"keywords\": \", \".join(c.get(\"keywords\", [])), # FIXED: stringify list\n",
    "            \"rarity\": c.get(\"rarity\", \"\")\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(docs)} cards from {path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e04fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER CHUNK TEXT --------\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Split text into smaller chunks so embeddings don't get too big.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current = []\n",
    "    length = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = len(s.split())\n",
    "        if length + tokens > chunk_size:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            length = tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            length += tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57272cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER BUILD INDEX --------\n",
    "def build_index():\n",
    "    \"\"\"Create ChromaDB collection from rules + card data.\"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    print(\"Loading rules...\")\n",
    "    rules = load_rules(RULES_FILE)\n",
    "\n",
    "    print(\"Loading cards...\")\n",
    "    cards = load_cards(CARDS_FILE)  # add this\n",
    "\n",
    "    all_docs = rules + cards  # merge datasets\n",
    "\n",
    "    texts, metas, ids = [], [], []\n",
    "\n",
    "    for d in all_docs:\n",
    "        chunks = chunk_text(d[\"text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            texts.append(ch)\n",
    "            metas.append(d)\n",
    "            ids.append(f\"{d['id']}_{i}\")\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid chunks found to embed.\")\n",
    "\n",
    "    print(f\"Total chunks: {len(texts)}\")\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    vecs = [d.embedding for d in embeddings.data]\n",
    "\n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "\n",
    "    # Drop old collection (clean rebuild)\n",
    "    try:\n",
    "        chroma_client.delete_collection(\"mtg_data\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_data\")\n",
    "\n",
    "    # Add to Chroma\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=vecs,\n",
    "        documents=texts,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    print(\"Index built and saved with ChromaDB!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94b43337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER SEARCH INDEX --------\n",
    "def search_index(query, top_k=TOP_K):\n",
    "    \"\"\"Search ChromaDB for relevant rule chunks.\"\"\"\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        raise ValueError(\"Empty query provided.\")\n",
    "\n",
    "    client = OpenAI()\n",
    "    emb = client.embeddings.create(model=EMBED_MODEL, input=[query])\n",
    "    vec = emb.data[0].embedding\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_rules\")\n",
    "\n",
    "    results = collection.query(query_embeddings=[vec], n_results=top_k)\n",
    "\n",
    "    docs = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        docs.append({\n",
    "            \"text\": doc,\n",
    "            \"meta\": results[\"metadatas\"][0][i]\n",
    "        })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9f62d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER GENERATE SUBQUERIES --------\n",
    "def generate_subqueries(query, n=10):\n",
    "    \"\"\"Chain of Thought decomposition function. Use the LLM to break a user query into smaller sub-questions.\"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    prompt = f\"\"\"\n",
    "    Break down the following Magic: The Gathering rules question into {n} smaller, \n",
    "    more specific sub-questions that cover timing, abilities, rules interactions, \n",
    "    and possible edge cases. Return them as a numbered list.\n",
    "\n",
    "    Original Question: {query}\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert MTG judge assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    text = resp.choices[0].message.content\n",
    "    subqueries = [line.strip(\"0123456789. \") for line in text.splitlines() if line.strip()]\n",
    "    return subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER ANSWER WITH SUBQUERIES --------\n",
    "def answer_with_subqueries(query):\n",
    "    \"\"\"Break question into subqueries, search index for each, and generate final answer.\"\"\"\n",
    "    # Step 1: Get subqueries\n",
    "    subqueries = generate_subqueries(query, n=10)\n",
    "    # print(\"Subqueries:\", subqueries)\n",
    "\n",
    "    # Step 2: Collect context from all subqueries\n",
    "    all_context = []\n",
    "    for sq in subqueries:\n",
    "        results = search_index(sq, top_k=3)  # use your existing search_index #todo test with 5\n",
    "        for i, r in enumerate(results, 1):\n",
    "            all_context.append(f\"Subquery: {sq}\\n- Source: {r['meta'].get('source', '')}\\n- Text: {r['text']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(all_context)\n",
    "\n",
    "    #todo add question in a clear format\n",
    "    # response_format = \"\"\"\n",
    "    # Please provide a structured answer with the following format:\n",
    "    # - Short Answer: [short summary answer, this sentence should always start with a Yes or No if possible]\n",
    "    # - Full Explanation: [detailed reasoning with rules and card interactions]\n",
    "    # - Sources: [cite the rule numbers and add the text of the cards involved or that were most relevant]\n",
    "    # \"\"\"\n",
    "\n",
    "    #! temporal reply format for benchmarks\n",
    "    response_format = \"Respond only with a Yes or No answer. Nothing more should be added\"\n",
    "\n",
    "    # Step 3: Ask LLM for final structured response\n",
    "    user_prompt = f\"\"\"\n",
    "    You are an expert Magic: The Gathering judge assistant.\n",
    "    A user has a question about card interactions or rules.\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Use these sources (rules + card texts):\n",
    "    {context}\n",
    "\n",
    "    Answer format:\n",
    "    {response_format}\n",
    "    \"\"\"\n",
    "\n",
    "    #todo add model call at the end to agree or disagree with the query, context and response\n",
    "    \n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0, # lowering temperature for more accurate and consistant response according to rules\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert MTG judge assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER ANSWER QUESTION -------- #! replaced by answer_with_subqueries\n",
    "# def answer_question(query):\n",
    "#     \"\"\"Retrieve context and ask the LLM for an MTG answer (TLDR + Explanation + Sources).\"\"\"\n",
    "#     results = search_index(query)\n",
    "#     context_blocks = []\n",
    "\n",
    "#     for i, r in enumerate(results, 1):\n",
    "#         text = r[\"text\"]\n",
    "#         meta = r[\"meta\"]\n",
    "#         source = meta.get(\"source\", \"Unknown\")\n",
    "#         context_blocks.append(f\"[{i}] ({source}) {text}\")\n",
    "\n",
    "#     context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "#     #! changed for before for benchmarks\n",
    "#     # response_format = \"\"\"\n",
    "#     # Please provide a structured answer with the following format:\n",
    "#     # - Short Answer: [short summary answer, this sentence should always start with a Yes or No if possible]\n",
    "#     # - Full Explanation: [detailed reasoning with rules and card interactions]\n",
    "#     # - Sources: [cite the rule numbers and add the text of the cards involved or that were most relevant]\n",
    "#     # \"\"\"\n",
    "\n",
    "#     response_format = \"Respond only with a Yes or No answer. Nothing more should be added\"\n",
    "\n",
    "#     user_prompt = f\"\"\"\n",
    "#     You are an expert Magic: The Gathering judge assistant.\n",
    "#     A user has a question about card interactions or rules.\n",
    "\n",
    "#     Question:\n",
    "#     {query}\n",
    "\n",
    "#     Use these sources (rules + card texts):\n",
    "#     {context}\n",
    "\n",
    "#     Answer format:\n",
    "#     {response_format}\n",
    "#     \"\"\"\n",
    "\n",
    "#     client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "#     resp = client.chat.completions.create(\n",
    "#         model=CHAT_MODEL,\n",
    "#         temperature=0,  \n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful MTG rules assistant. Always explain clearly.\"},\n",
    "#             {\"role\": \"user\", \"content\": user_prompt}\n",
    "#         ]\n",
    "#     )\n",
    "#     return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07ad31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rules...\n",
      "Loading cards...\n",
      "Loaded 90 cards from ./clean-standard-cards.json\n",
      "Total chunks: 91\n",
      "Index built and saved with ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# -------- BUILDING INDEX --------\n",
    "build_index()  # only first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd13c42",
   "metadata": {},
   "source": [
    "# Multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c00ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct answers: 39/45\n",
      "Does scry let you look at cards and choose to put some on top or bottom of your library?. Right answer: Yes. Response: No\n",
      "Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?. Right answer: Yes. Response: No\n",
      "Are continuous effects applied in a specific layered system, such as type-changing, ability additions, P/T changes, in numbered order?. Right answer: Yes. Response: No\n",
      "Are special actions like playing a land or turning a face-down creature face-up things opponents cannot respond to?. Right answer: Yes. Response: No\n",
      "If I imprint Time Walk on Panoptic Mirror, do I get infinite turns?. Right answer: Yes. Response: No\n",
      "Someone is playing Flashfreeze on one of my spells, Can I play Aven Interrupter on top of Flashfreeze so my initial spell can be resolved?. Right answer: Yes. Response: No\n"
     ]
    }
   ],
   "source": [
    "with open(\"easy-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    easy_questions = json.load(f)\n",
    "with open(\"hard-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_questions = json.load(f)\n",
    "with open(\"own-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    own_questions = json.load(f)\n",
    "\n",
    "all_questions = easy_questions + hard_questions + own_questions\n",
    "\n",
    "correct_answers = 0\n",
    "wrong_answered_questions = []\n",
    "\n",
    "for question in all_questions:\n",
    "    # response = answer_question(question[\"text\"])\n",
    "    response = answer_with_subqueries(question[\"text\"])\n",
    "    # response = answer_with_subqueries(question.text)\n",
    "    if response == question[\"answer\"]:\n",
    "        correct_answers += 1\n",
    "    else:\n",
    "        wrong_answered_questions.append(f\"{question[\"text\"]}. Right answer: {question[\"answer\"]}. Response: {response}\")\n",
    "\n",
    "print(f\"correct answers: {correct_answers}/{len(all_questions)}\")\n",
    "for each_wrong_question in wrong_answered_questions:\n",
    "    print(each_wrong_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d4e00",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812b32e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Test: No temperature set and no subqueries.\n",
    "\n",
    "### correct answers: 30/45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2a0e5",
   "metadata": {},
   "source": [
    "### Test: Temperature set to 0.1 and no subqueries.\n",
    "\n",
    "### correct answers: 35/45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33259b",
   "metadata": {},
   "source": [
    "### Test: Temperature set to 0.1 and 5 subqueries.\n",
    "\n",
    "### Correct answers: 37/45\n",
    "\n",
    "### incorrect answer to:\n",
    "- Does scry let you look at cards and choose to put some on top or bottom of your library?\n",
    "- If you draw more than seven cards, can you keep them all if no other effect limits your hand size?\n",
    "- Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?\n",
    "- If a creature phases out and back in, does it lose summoning sickness if it had it before?\n",
    "- Are special actions like playing a land or turning a face-down creature face-up things opponents cannot respond to?\n",
    "- If I imprint Time Walk on Panoptic Mirror, do I get infinite turns?\n",
    "- I am being attacked by Axebane Ferox and I declare Aegis Turtle as a blocker. But before assigning damage, I play Bounce Off on Aegis Turtle. Do I still receive 4 damage from Agonasaur Rex?\n",
    "- Someone is playing Flashfreeze on one of my spells, Can I play Aven Interrupter on top of Flashfreeze so my initial spell can be resolved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5de3c",
   "metadata": {},
   "source": [
    "### Test: Temperature set to 0 and 10 subqueries.\n",
    "\n",
    "# 8 minutes for 45 queries (10 seconds per query)\n",
    "\n",
    "# correct answers: 39/45\n",
    "\n",
    "### incorrect answer to:\n",
    "- Does scry let you look at cards and choose to put some on top or bottom of your library?. Right answer: Yes. Response: No\n",
    "- Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?. Right answer: Yes. Response: No\n",
    "- Are continuous effects applied in a specific layered system, such as type-changing, ability additions, P/T changes, in numbered order?. Right answer: Yes. Response: No\n",
    "- Are special actions like playing a land or turning a face-down creature face-up things opponents cannot respond to?. Right answer: Yes. Response: No\n",
    "- If I imprint Time Walk on Panoptic Mirror, do I get infinite turns?. Right answer: Yes. Response: No\n",
    "- Someone is playing Flashfreeze on one of my spells, Can I play Aven Interrupter on top of Flashfreeze so my initial spell can be resolved?. Right answer: Yes. Response: No"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
