{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f40982b",
   "metadata": {},
   "source": [
    "MTG Judge RAG\n",
    "---------------------------------------------------\n",
    "This is a simple Python script for building an AI-powered MTG rules assistant\n",
    "using Retrieval-Augmented Generation (RAG) with OpenAI + ChromaDB.\n",
    "\n",
    "- Loads the Comprehensive Rules from a text file.\n",
    "- Splits rules into chunks.\n",
    "- Creates embeddings with OpenAI.\n",
    "- Stores them in ChromaDB for fast search (not using FAISS due to py version conflict)\n",
    "- Lets you ask questions, retrieves relevant rules, and asks the LLM to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe92255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- IMPORTS --------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import chromadb\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBED_MODEL = \"text-embedding-3-large\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "CHROMA_DB_DIR = \"../chroma_db\"\n",
    "os.makedirs(CHROMA_DB_DIR, exist_ok=True) # to create folder if it doesn't exist\n",
    "RULES_FILE = \"../data/comprehensive-rules.txt\"\n",
    "CARDS_FILE = \"../data/clean-standard-cards.json\"\n",
    "CHUNK_SIZE = 700 # words approximation\n",
    "TOP_K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4a9002e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- INITIALIZATION --------\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "baf4fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER LOAD RULES --------\n",
    "def load_rules(path):\n",
    "    \"\"\"Load the MTG comprehensive rules from a text file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Rules file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    docs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Rules usually like: 603.1. Some text\n",
    "            match = re.match(r\"^(\\d{1,3}(?:\\.\\d+)+)\\s+(.*)$\", line)\n",
    "            if match:\n",
    "                rule_id, body = match.groups()\n",
    "                docs.append({\n",
    "                    \"id\": f\"CR:{rule_id}\",\n",
    "                    \"text\": f\"{rule_id} {body}\",\n",
    "                    \"rule_id\": rule_id,\n",
    "                    \"source\": \"Comprehensive Rules\"\n",
    "                })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb4bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER LOAD CARDS --------\n",
    "def load_cards(path):\n",
    "    \"\"\"Load MTG card data from your JSON export.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Card file not found at {path}\")\n",
    "        return []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cards = json.load(f)\n",
    "\n",
    "    docs = []\n",
    "    for c in cards:\n",
    "        # Skip cards without names or text\n",
    "        if \"name\" not in c or not c.get(\"originalText\"):\n",
    "            continue\n",
    "\n",
    "        # Construct a searchable text block for embedding\n",
    "        text_parts = [\n",
    "            f\"Name: {c['name']}\",\n",
    "            f\"Mana Cost: {c.get('manaCost', '')}\",\n",
    "            f\"Types: {' '.join(c.get('types', []))}\",\n",
    "            f\"Subtypes: {' '.join(c.get('subtypes', []))}\",\n",
    "            f\"Abilities/Keywords: {', '.join(c.get('keywords', []))}\",\n",
    "            f\"Text: {c['originalText']}\"\n",
    "        ]\n",
    "\n",
    "        # Add rulings (big chunk but useful)\n",
    "        rulings = c.get(\"rulings\", [])\n",
    "        if rulings:\n",
    "            rulings_text = \" | \".join(r[\"text\"] for r in rulings if \"text\" in r)\n",
    "            text_parts.append(f\"Rulings: {rulings_text}\")\n",
    "\n",
    "        full_text = \"\\n\".join(text_parts)\n",
    "\n",
    "        docs.append({\n",
    "            \"id\": f\"CARD:{c['uuid']}\",   # use UUID for uniqueness\n",
    "            \"text\": full_text,\n",
    "            \"source\": \"Card Database\",\n",
    "            \"card_name\": c[\"name\"],\n",
    "            \"manaCost\": c.get(\"manaCost\", \"\"),\n",
    "            \"types\": \", \".join(c.get(\"types\", [])),       # FIXED: stringify list\n",
    "            \"subtypes\": \", \".join(c.get(\"subtypes\", [])), # FIXED: stringify list\n",
    "            \"keywords\": \", \".join(c.get(\"keywords\", [])), # FIXED: stringify list\n",
    "            \"rarity\": c.get(\"rarity\", \"\")\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(docs)} cards from {path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e04fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER CHUNK TEXT --------\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Split text into smaller chunks so embeddings don't get too big.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current = []\n",
    "    length = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = len(s.split())\n",
    "        if length + tokens > chunk_size:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [s]\n",
    "            length = tokens\n",
    "        else:\n",
    "            current.append(s)\n",
    "            length += tokens\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57272cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER BUILD INDEX --------\n",
    "def build_index():\n",
    "    \"\"\"Create ChromaDB collection from rules + card data.\"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    print(\"Loading rules...\")\n",
    "    rules = load_rules(RULES_FILE)\n",
    "\n",
    "    print(\"Loading cards...\")\n",
    "    cards = load_cards(CARDS_FILE)  # add this\n",
    "\n",
    "    all_docs = rules + cards  # merge datasets\n",
    "\n",
    "    texts, metas, ids = [], [], []\n",
    "\n",
    "    for d in all_docs:\n",
    "        chunks = chunk_text(d[\"text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            texts.append(ch)\n",
    "            metas.append(d)\n",
    "            ids.append(f\"{d['id']}_{i}\")\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid chunks found to embed.\")\n",
    "\n",
    "    print(f\"Total chunks: {len(texts)}\")\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    vecs = [d.embedding for d in embeddings.data]\n",
    "\n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "\n",
    "    # Drop old collection (clean rebuild)\n",
    "    try:\n",
    "        chroma_client.delete_collection(\"mtg_data\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_data\")\n",
    "\n",
    "    # Add to Chroma\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=vecs,\n",
    "        documents=texts,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "    print(\"Index built and saved with ChromaDB!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "94b43337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER SEARCH INDEX --------\n",
    "def search_index(query, top_k=TOP_K):\n",
    "    \"\"\"Search ChromaDB for relevant rule chunks.\"\"\"\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        raise ValueError(\"Empty query provided.\")\n",
    "\n",
    "    # client = OpenAI()\n",
    "    emb = client.embeddings.create(model=EMBED_MODEL, input=[query])\n",
    "    vec = emb.data[0].embedding\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"mtg_rules\")\n",
    "\n",
    "    results = collection.query(query_embeddings=[vec], n_results=top_k)\n",
    "\n",
    "    docs = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        docs.append({\n",
    "            \"text\": doc,\n",
    "            \"meta\": results[\"metadatas\"][0][i]\n",
    "        })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9f62d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER GENERATE SUBQUERIES --------\n",
    "def generate_subqueries(query, n=10):\n",
    "    \"\"\"Chain of Thought decomposition function. Use the LLM to break a user query into smaller sub-questions.\"\"\"\n",
    "    #client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    prompt = f\"\"\"\n",
    "    Break down the following Magic: The Gathering rules question into {n} smaller, \n",
    "    more specific sub-questions that cover timing, abilities, rules interactions, \n",
    "    and possible edge cases. Return them as a numbered list.\n",
    "\n",
    "    Original Question: {query}\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert MTG judge assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    text = resp.choices[0].message.content\n",
    "    subqueries = [line.strip(\"0123456789. \") for line in text.splitlines() if line.strip()]\n",
    "    return subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "60d0491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- HELPER JSON PARSE --------\n",
    "def safe_json_parse(text):\n",
    "    \"\"\"Function to safely parse from string to json, even if wrapped in markdown fences. Converts response from gpt to json\"\"\"\n",
    "    fixed = text.strip()\n",
    "\n",
    "    # Strip markdown fences if present\n",
    "    if fixed.startswith(\"```\"):\n",
    "        # Remove the first line (``` or ```json)\n",
    "        fixed = \"\\n\".join(fixed.split(\"\\n\")[1:])\n",
    "        # Remove trailing fence\n",
    "        if fixed.strip().endswith(\"```\"):\n",
    "            fixed = \"\\n\".join(fixed.strip().split(\"\\n\")[:-1])\n",
    "\n",
    "    # Try JSON decode\n",
    "    try:\n",
    "        return json.loads(fixed)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\", \"raw\": text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- ANSWER WITH SUBQUERIES ----------\n",
    "def answer_with_subqueries(query, max_context_chunks=20, max_subqueries=20):\n",
    "    \"\"\"Break question into subqueries, search index for each, and generate final structured ruling.\"\"\"\n",
    "\n",
    "    # Step 1: Generate subqueries\n",
    "    subqueries = generate_subqueries(query, n=max_subqueries)\n",
    "\n",
    "    # Step 2: Collect retrieval results\n",
    "    all_results = []\n",
    "    for sq in subqueries:\n",
    "        results = search_index(sq, top_k=8)\n",
    "        for r in results:\n",
    "            all_results.append({\n",
    "                \"subquery\": sq,\n",
    "                \"source\": r[\"meta\"].get(\"source\", \"\"),\n",
    "                \"text\": r[\"text\"]\n",
    "            })\n",
    "\n",
    "    # Prune context if too large (keep only top N chunks by length relevance)\n",
    "    if len(all_results) > max_context_chunks:\n",
    "        all_results = all_results[:max_context_chunks]\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"Subquery: {r['subquery']}\\n- Source: {r['source']}\\n- Text: {r['text']}\"\n",
    "        for r in all_results\n",
    "    )\n",
    "\n",
    "    # Response format instructions\n",
    "    response_format = \"\"\"\n",
    "    Provide a structured JSON with the following fields:\n",
    "\n",
    "    - \"question\": rephrased user question,\n",
    "    - \"single_word_answer\": \"Yes\", \"No\" or \"Unclear\",\n",
    "    - \"short_answer\": short paragraph summary,\n",
    "    - \"full_explanation\": detailed reasoning with rules and card interactions,\n",
    "    - \"sources\": cite only the rules/cards you actually used, not everything retrieved.\n",
    "    \"\"\"\n",
    "\n",
    "    # System + user prompt for judge #1\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert Magic: The Gathering judge assistant.\n",
    "\n",
    "    Sources available (rules + card texts):\n",
    "    {context}\n",
    "\n",
    "    Answer format:\n",
    "    {response_format}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    The user's question is:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    #* Initial judge calling\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"} \n",
    "    )\n",
    "\n",
    "    judge1_answer = resp.choices[0].message.content\n",
    "\n",
    "    # ---------- SECONDARY JUDGE ----------\n",
    "    judge2_system_prompt = f\"\"\"\n",
    "    You are an expert MTG high judge reviewing another judge’s ruling.\n",
    "\n",
    "    You will be given:\n",
    "    - User's question\n",
    "    - Judge’s ruling\n",
    "    - The context they used\n",
    "\n",
    "    If you agree: reply only with \"Accepted\".\n",
    "    If you disagree: reply with \"Denied, [reason + extra context suggestions]\".\n",
    "    \"\"\"\n",
    "\n",
    "    judge_prompt = f\"\"\"\n",
    "    User Question:\n",
    "    {query}\n",
    "\n",
    "    Context Used:\n",
    "    {context}\n",
    "\n",
    "    Judge's Ruling:\n",
    "    {judge1_answer}\n",
    "    \"\"\"\n",
    "\n",
    "    #* Secondary judge calling\n",
    "    resp2 = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge2_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": judge_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    judge2_response = resp2.choices[0].message.content.strip()\n",
    "\n",
    "    # ---------- ACCEPTED CASE ----------\n",
    "    if judge2_response.startswith(\"Accepted\"):\n",
    "        return safe_json_parse(judge1_answer)\n",
    "\n",
    "    # ---------- DENIED CASE ----------\n",
    "    # Generate refined subqueries based on judge2 feedback\n",
    "    new_subqueries = generate_subqueries(judge2_response, n=max_subqueries)\n",
    "    print(\"2nd judge conflict\")\n",
    "\n",
    "    refined_results = []\n",
    "    for sq in new_subqueries:\n",
    "        results = search_index(sq, top_k=5)\n",
    "        for r in results:\n",
    "            refined_results.append({\n",
    "                \"subquery\": sq,\n",
    "                \"source\": r[\"meta\"].get(\"source\", \"\"),\n",
    "                \"text\": r[\"text\"]\n",
    "            })\n",
    "\n",
    "    # Keep within limits\n",
    "    if len(refined_results) > max_context_chunks:\n",
    "        refined_results = refined_results[:max_context_chunks]\n",
    "\n",
    "    refined_context = \"\\n\\n\".join(\n",
    "        f\"Subquery: {r['subquery']}\\n- Source: {r['source']}\\n- Text: {r['text']}\"\n",
    "        for r in refined_results\n",
    "    )\n",
    "\n",
    "    new_prompt = f\"\"\"\n",
    "    A higher judge denied your ruling for lack of context. Use the new context and improve your ruling.\n",
    "\n",
    "    Higher judge feedback:\n",
    "    {judge2_response}\n",
    "\n",
    "    New context:\n",
    "    {refined_context}\n",
    "\n",
    "    Use the same JSON format as before.\n",
    "    \"\"\"\n",
    "\n",
    "    #* Loop back to initial judge\n",
    "    resp3 = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": judge1_answer},\n",
    "            {\"role\": \"user\", \"content\": new_prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"} \n",
    "    )\n",
    "\n",
    "    return safe_json_parse(resp3.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # return {\n",
    "        #     \"single_word_answer\": \"Denied\",\n",
    "        #     \"question\": query,\n",
    "        #     \"full_explanation\": f\"\"\"\n",
    "        #         We’re sorry, our virtual judges were unable to reach an agreement on a final response to your question.\n",
    "\n",
    "        #         Reason: The original judge’s ruling was: {judge2_response}\n",
    "\n",
    "        #         Please try asking your question again, this time with clearer wording to help us provide a more definitive answer.\n",
    "        #     \"\"\"\n",
    "        # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "07ad31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rules...\n",
      "Loading cards...\n",
      "Loaded 90 cards from ./clean-standard-cards.json\n",
      "Total chunks: 91\n",
      "Index built and saved with ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# -------- BUILDING INDEX --------\n",
    "build_index()  # only first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c76f5f",
   "metadata": {},
   "source": [
    "# Single testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "46b4e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?\",\n",
      "    \"single_word_answer\": \"No\",\n",
      "    \"short_answer\": \"There cannot be infinite or multiple cleanup steps in Magic: The Gathering. The cleanup step is a single step in the turn structure, and while effects may trigger during it, they do not create additional cleanup steps.\",\n",
      "    \"full_explanation\": \"In Magic: The Gathering, the cleanup step is a defined part of the turn structure where players discard down to their maximum hand size and any end-of-turn effects are resolved. According to the Comprehensive Rules, there is only one cleanup step per turn. While effects like Kozilek, Butcher of Truth may trigger discard effects, they do not create additional cleanup steps. If a player has to discard and has no cards in hand, they simply do not discard anything, and the cleanup step proceeds to the next phase of the turn. Therefore, even if multiple effects are triggered during the cleanup step, they do not lead to infinite or multiple cleanup steps.\",\n",
      "    \"sources\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "single_question = \"Can there be infinite or multiple cleanup steps triggered by effects like Kozilek plus discard effects?\"\n",
    "response = answer_with_subqueries(single_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd13c42",
   "metadata": {},
   "source": [
    "# Multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct answers: 42/45\n",
      "judge ruling conflict: 0/45\n",
      "unclear questions: 0/45\n",
      "incorrect answers: 3/45\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/easy-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    easy_questions = json.load(f)\n",
    "with open(\"../data/hard-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_questions = json.load(f)\n",
    "with open(\"../data/own-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    own_questions = json.load(f)\n",
    "# with open(\"../data/wrong-questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     wrong_questions = json.load(f)\n",
    "\n",
    "all_questions = easy_questions + hard_questions + own_questions\n",
    "# all_questions = wrong_questions\n",
    "\n",
    "correct_answers = []\n",
    "judge_ruling_conflict_questions = []\n",
    "unclear_questions = []\n",
    "incorrect_questions = []\n",
    "\n",
    "for question in all_questions:\n",
    "    response = answer_with_subqueries(question[\"text\"])\n",
    "\n",
    "    gold = question[\"answer\"].strip().lower()\n",
    "    pred = response[\"single_word_answer\"].strip().lower()\n",
    "\n",
    "    if pred == gold:\n",
    "        correct_answers.append({\"question\": question, \"response\": response})\n",
    "    elif pred == \"denied\":\n",
    "        judge_ruling_conflict_questions.append({\"question\": question, \"response\": response})\n",
    "    elif pred == \"unclear\":\n",
    "        unclear_questions.append({\"question\": question, \"response\": response})\n",
    "    else:\n",
    "        incorrect_questions.append({\"question\": question, \"response\": response})\n",
    "\n",
    "total_question_length = len(all_questions)\n",
    "\n",
    "print(f\"correct answers: {len(correct_answers)}/{total_question_length}\")\n",
    "\n",
    "print(f\"judge ruling conflict: {len(judge_ruling_conflict_questions)}/{total_question_length}\")\n",
    "# for question in judge_ruling_conflict_questions:\n",
    "#     print(question)\n",
    "\n",
    "print(f\"unclear questions: {len(unclear_questions)}/{total_question_length}\")\n",
    "# for question in unclear_questions:\n",
    "#     print(question)\n",
    "\n",
    "print(f\"incorrect answers: {total_question_length - len(correct_answers) - len(judge_ruling_conflict_questions) - len(unclear_questions)}/{total_question_length}\")\n",
    "# for question in incorrect_questions:\n",
    "#     print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4884ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in incorrect_questions:\n",
    "    print(\"original question:\", question[\"question\"][\"text\"])\n",
    "    print(\"correct answer: \", question[\"question\"][\"answer\"])\n",
    "    print(\"reworded question: \", question[\"response_dict\"][\"question\"])\n",
    "    print(\"single_word_answer: \", question[\"response_dict\"][\"single_word_answer\"])\n",
    "    print(\"short_answer: \", question[\"response_dict\"][\"short_answer\"])\n",
    "    print(\"full_explanation: \", question[\"response_dict\"][\"full_explanation\"])\n",
    "    print(\"sources: \", question[\"response_dict\"][\"sources\"])\n",
    "    print(\"-------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
